# G√©n√©ration de QCM avec IA - Documentation

## üìö Vue d'ensemble

Le syst√®me AI-KO int√®gre une g√©n√©ration automatique de questions de QCM utilisant **Qwen2.5-7B-Instruct**, un mod√®le de langage open-source de pointe, via l'API Hugging Face Inference.

## üéØ Mod√®le IA Utilis√©

### Qwen2.5-7B-Instruct

- **D√©veloppeur:** Alibaba Cloud
- **Taille:** 7 milliards de param√®tres
- **Type:** Mod√®le de g√©n√©ration de texte instruction-tuned
- **Points forts:**
  - Excellente compr√©hension du fran√ßais
  - G√©n√©ration structur√©e (JSON, XML, etc.)
  - Raisonnement logique et p√©dagogique
  - Tr√®s rapide via API Inference

**Pourquoi ce mod√®le?**
- ‚úÖ Open-source (licence permissive)
- ‚úÖ Performances comparables √† GPT-3.5
- ‚úÖ Excellente pour la g√©n√©ration √©ducative
- ‚úÖ API gratuite avec token Hugging Face
- ‚úÖ Pas de t√©l√©chargement local n√©cessaire

## üîß Configuration

### 1. Obtenir un Token Hugging Face

1. Cr√©ez un compte sur [Hugging Face](https://huggingface.co)
2. Allez dans [Settings > Access Tokens](https://huggingface.co/settings/tokens)
3. Cr√©ez un nouveau token avec les permissions de lecture
4. Copiez le token

### 2. Configurer le Token

Ajoutez le token dans votre fichier `.env` :

```bash
# Token Hugging Face pour l'API Inference
HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxxxxxxxxxx
```

### 3. V√©rifier les D√©pendances

Les d√©pendances suivantes sont requises (d√©j√† dans `requirements.txt`) :

```bash
# Extraction de documents
PyPDF2==3.0.1
python-docx==1.1.2

# API HTTP
requests==2.32.3

# Celery pour l'asynchrone
celery==5.4.0
redis==5.2.1
```

## üöÄ Utilisation

### Workflow de G√©n√©ration

```
1. Enseignant cr√©e un QCM ‚Üí Frontend
2. Enseignant fournit texte/document ‚Üí Frontend
3. Backend cr√©e QCM vide en DB
4. T√¢che Celery lanc√©e (asynchrone)
5. Extraction du texte (si document)
6. Nettoyage et pr√©paration du texte
7. Appel API Hugging Face (Qwen2.5)
8. Parsing et validation des questions
9. Enregistrement en base de donn√©es
10. Notification frontend (polling)
```

### API Endpoints

#### G√©n√©ration depuis Texte

```http
POST /api/qcm/generate/text
Content-Type: application/json
Authorization: Bearer <jwt_token>

{
  "titre": "QCM de Math√©matiques",
  "text": "Le th√©or√®me de Pythagore stipule que...",
  "num_questions": 10,
  "matiere": "Math√©matiques",
  "niveau": "L1",
  "duree": 60
}
```

**R√©ponse:**
```json
{
  "task_id": "abc-123-def",
  "status": "PENDING",
  "qcm_id": "qcm-uuid",
  "message": "G√©n√©ration en cours..."
}
```

#### G√©n√©ration depuis Document

```http
POST /api/qcm/generate/document
Content-Type: application/json
Authorization: Bearer <jwt_token>

{
  "titre": "QCM sur le Chapitre 5",
  "file_content": "<base64_encoded_file>",
  "file_type": "pdf",
  "num_questions": 15,
  "matiere": "Physique",
  "niveau": "L2"
}
```

#### V√©rifier le Statut de la T√¢che

```http
GET /api/qcm/tasks/{task_id}
Authorization: Bearer <jwt_token>
```

**R√©ponses possibles:**

```json
// En cours
{
  "task_id": "abc-123",
  "status": "PROGRESS",
  "result": {
    "status": "G√©n√©ration des questions avec l'IA...",
    "progress": 40
  }
}

// Succ√®s
{
  "task_id": "abc-123",
  "status": "SUCCESS",
  "result": {
    "qcm_id": "qcm-uuid",
    "titre": "QCM de Math√©matiques",
    "num_questions": 10,
    "status": "success",
    "message": "QCM g√©n√©r√© avec succ√®s: 10 questions cr√©√©es"
  }
}

// Erreur
{
  "task_id": "abc-123",
  "status": "FAILURE",
  "error": "Le mod√®le met trop de temps √† charger..."
}
```

## üß† Architecture du Service IA

### 1. Document Parser (`app/services/document_parser.py`)

**Responsabilit√©s:**
- Extraction de texte depuis PDF (PyPDF2)
- Extraction de texte depuis DOCX (python-docx)
- Nettoyage et normalisation du texte
- Troncature √† 8000 caract√®res (limite du mod√®le)

**Exemple:**
```python
from app.services.document_parser import DocumentParser

# Extraire depuis PDF
text = DocumentParser.extract_from_pdf(pdf_bytes)

# Nettoyer le texte
clean_text = DocumentParser.clean_text(text, max_length=8000)
```

### 2. AI Service (`app/services/ai_service.py`)

**Responsabilit√©s:**
- Construction du prompt pour Qwen2.5
- Appel API Hugging Face Inference
- Parsing JSON depuis la r√©ponse du mod√®le
- Validation des questions g√©n√©r√©es
- Gestion des erreurs et retries

**Configuration:**
```python
class AIService:
    model = "Qwen/Qwen2.5-7B-Instruct"
    api_url = "https://api-inference.huggingface.co/models/{model}"
    max_retries = 3
    timeout = 60
```

**Prompt Engineering:**

Le service construit un prompt structur√© avec :
- Contexte (mati√®re, niveau)
- Texte source
- Instructions claires pour le format JSON
- R√®gles de validation (4 options, 1 correcte, etc.)

### 3. T√¢ches Celery (`app/tasks/quiz_generation.py`)

**T√¢ches asynchrones:**

- `generate_quiz_from_text`: G√©n√©ration depuis texte brut
- `generate_quiz_from_document`: G√©n√©ration depuis PDF/DOCX

**√âtats de progression:**
1. `PROGRESS(10%)` - Analyse du texte
2. `PROGRESS(30%)` - G√©n√©ration IA
3. `PROGRESS(70%)` - Enregistrement DB
4. `SUCCESS(100%)` - Termin√©

## üìã Format des Questions G√©n√©r√©es

### Structure JSON

```json
{
  "questions": [
    {
      "enonce": "Quelle est la formule du th√©or√®me de Pythagore?",
      "type": "qcm",
      "options": [
        {
          "texte": "a¬≤ + b¬≤ = c¬≤",
          "estCorrecte": true
        },
        {
          "texte": "a + b = c",
          "estCorrecte": false
        },
        {
          "texte": "a¬≤ - b¬≤ = c¬≤",
          "estCorrecte": false
        },
        {
          "texte": "a √ó b = c",
          "estCorrecte": false
        }
      ],
      "explication": "Le th√©or√®me de Pythagore stipule que dans un triangle rectangle, le carr√© de l'hypot√©nuse est √©gal √† la somme des carr√©s des deux autres c√¥t√©s.",
      "points": 1
    }
  ]
}
```

### Validation Automatique

Le service v√©rifie:
- ‚úÖ √ânonc√© non vide
- ‚úÖ Au moins 2 options
- ‚úÖ Exactement 1 option correcte
- ‚úÖ Options non vides
- ‚úÖ JSON valide

Si une question ne respecte pas les r√®gles, elle est corrig√©e ou ignor√©e.

## ‚öôÔ∏è Configuration Avanc√©e

### Modifier le Mod√®le

Pour utiliser un autre mod√®le Hugging Face, modifiez `ai_service.py` :

```python
class AIService:
    # Mod√®le alternatif : Mistral-7B-Instruct
    self.model = "mistralai/Mistral-7B-Instruct-v0.2"

    # Ou : Meta-Llama-3-8B-Instruct (n√©cessite approbation)
    # self.model = "meta-llama/Meta-Llama-3-8B-Instruct"
```

### Ajuster les Param√®tres de G√©n√©ration

```python
payload = {
    "inputs": prompt,
    "parameters": {
        "max_new_tokens": 2048,      # Longueur max de la r√©ponse
        "temperature": 0.7,           # Cr√©ativit√© (0.1-1.0)
        "top_p": 0.9,                 # Nucleus sampling
        "do_sample": True,            # Activer l'√©chantillonnage
        "return_full_text": False     # Ne retourner que le nouveau texte
    }
}
```

**Recommandations:**
- `temperature = 0.7` : Bon √©quilibre pour les QCM
- `temperature < 0.5` : Plus d√©terministe, questions plus factuelles
- `temperature > 0.8` : Plus cr√©atif, questions plus vari√©es

### Gestion des Erreurs

**Erreur 503 - Mod√®le en chargement:**
- Le service attend 10 secondes et r√©essaie
- 3 tentatives maximum
- Message : "Le mod√®le met trop de temps √† charger"

**Solution:** R√©essayer apr√®s quelques minutes ou utiliser un mod√®le plus l√©ger.

## üîç D√©bogage

### Activer les Logs D√©taill√©s

```python
# config.py
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger('app.services.ai_service')
logger.setLevel(logging.DEBUG)
```

### Tester le Service IA Directement

```python
from app.services.ai_service import ai_service

text = """
Le th√©or√®me de Pythagore est fondamental en g√©om√©trie.
Il s'applique aux triangles rectangles.
"""

questions = ai_service.generate_questions(
    text=text,
    num_questions=3,
    matiere="Math√©matiques",
    niveau="L1"
)

print(f"{len(questions)} questions g√©n√©r√©es:")
for q in questions:
    print(f"- {q['enonce']}")
```

### Tester l'Extraction de Documents

```python
from app.services.document_parser import DocumentParser

with open('cours.pdf', 'rb') as f:
    pdf_bytes = f.read()

text = DocumentParser.extract_from_pdf(pdf_bytes)
print(f"Texte extrait: {len(text)} caract√®res")
```

## üìä Performances

### Temps de G√©n√©ration Estim√©s

| Nombre de Questions | Temps Moyen |
|---------------------|-------------|
| 5 questions         | 15-30s      |
| 10 questions        | 30-60s      |
| 15 questions        | 45-90s      |
| 20 questions        | 60-120s     |

**Facteurs d'influence:**
- Charge du serveur Hugging Face
- Longueur du texte source
- Complexit√© du sujet

### Optimisations

1. **Texte pr√©-nettoy√©:** Limite √† 8000 caract√®res
2. **Retries intelligents:** 3 tentatives avec attente
3. **Timeout adaptatif:** 60 secondes par appel
4. **Cache mod√®le:** (c√¥t√© Hugging Face)

## üö® Limitations

### Limitations Actuelles

1. **Taille du texte:** Max 8000 caract√®res (limite Qwen2.5)
2. **Nombre de questions:** Max 20 par g√©n√©ration (pour maintenir la qualit√©)
3. **Types de questions:** Uniquement QCM √† 4 options
4. **Langues:** Optimis√© pour le fran√ßais (supporte anglais)

### Am√©liorations Futures

- [ ] Support de questions ouvertes
- [ ] Support de questions vrai/faux
- [ ] G√©n√©ration d'images avec DALL-E
- [ ] Analyse de qualit√© des questions
- [ ] Suggestions de difficult√©s

## üìù Exemples d'Utilisation

### Exemple 1: G√©n√©ration Depuis Texte (Python)

```python
import requests

url = "http://localhost:5000/api/qcm/generate/text"
headers = {"Authorization": "Bearer <token>"}

data = {
    "titre": "QCM Python D√©butant",
    "text": """
    Python est un langage de programmation interpr√©t√©.
    Il utilise l'indentation pour structurer le code.
    Les variables n'ont pas besoin de d√©claration de type.
    """,
    "num_questions": 5,
    "matiere": "Informatique",
    "niveau": "L1"
}

response = requests.post(url, json=data, headers=headers)
task_id = response.json()['task_id']

# Polling du statut
import time
while True:
    status_response = requests.get(
        f"http://localhost:5000/api/qcm/tasks/{task_id}",
        headers=headers
    )
    status = status_response.json()

    if status['status'] == 'SUCCESS':
        print("QCM g√©n√©r√© avec succ√®s!")
        print(status['result'])
        break
    elif status['status'] == 'FAILURE':
        print("Erreur:", status['error'])
        break

    time.sleep(2)
```

### Exemple 2: G√©n√©ration Depuis Document (Frontend)

```typescript
// D√©j√† impl√©ment√© dans QCMGenerateForm.tsx
const file = fileInput.files[0]
const base64 = await qcmService.fileToBase64(file)

const response = await qcmService.generateFromDocument({
  titre: "QCM Chapitre 5",
  file_content: base64,
  file_type: "pdf",
  num_questions: 10,
  matiere: "Math√©matiques",
  niveau: "L2"
})

// Polling avec useTaskPolling hook
const { progress, taskStatus } = useTaskPolling({
  taskId: response.task_id,
  onSuccess: (result) => {
    router.push(`/enseignant/qcm/${result.qcm_id}`)
  }
})
```

## üîí S√©curit√©

### Bonnes Pratiques

1. **Token HF_API_TOKEN:** Ne JAMAIS commiter dans le code
2. **Validation c√¥t√© serveur:** Toujours valider les fichiers upload√©s
3. **Limite de taille:** Max 10 Mo pour les documents
4. **Rate limiting:** Limiter les appels API par utilisateur
5. **Timeouts:** Configurer des timeouts appropri√©s

### Variables d'Environnement Sensibles

```bash
# .env
HF_API_TOKEN=hf_xxxxxxxxxxxxxxxxxxxxx  # ‚ö†Ô∏è NE PAS COMMITER

# ‚úÖ Utiliser des secrets management en production
# - AWS Secrets Manager
# - Azure Key Vault
# - HashiCorp Vault
```

## üìû Support

### Probl√®mes Courants

**Probl√®me:** "HF_API_TOKEN non d√©fini"
- **Solution:** Configurer le token dans `.env`

**Probl√®me:** "Le mod√®le met trop de temps √† charger"
- **Solution:** R√©essayer apr√®s 5-10 minutes (le mod√®le se met en cache)

**Probl√®me:** "Impossible de parser la r√©ponse du mod√®le en JSON"
- **Solution:** Le mod√®le peut avoir g√©n√©r√© du texte invalide. R√©essayer ou ajuster le prompt.

**Probl√®me:** "Timeout lors de la g√©n√©ration"
- **Solution:** R√©duire le nombre de questions ou la taille du texte

---

**üéì AI-KO - G√©n√©ration Intelligente de QCM**
